{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39deff2a-39f2-4fae-9270-0ed7b1696e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1:\n",
    "# Baggaing helps in reduce in overfitting by following ways:\n",
    "# 1. Bootstrapping: Bagging starts by creating multiple random subsets of the original training data through a process called bootstrapping. \n",
    "# Bootstrapping involves randomly sampling the training data with replacement\n",
    "# 2.Training Multiple Models: After creating the bootstrapped subsets, \n",
    "#   multiple decision tree models are trained on each of these subsets independently\n",
    "# 3.Aggregation: Once all the decision trees are trained, bagging combines their predictions to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f0c6c00-7b77-415b-b9b1-18230988043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 2:\n",
    "# Advantages\n",
    "# 1.Easy to understand and interpret.\n",
    "# 2.Can handle both numerical and categorical features.\n",
    "# 3.Nonlinear relationships can be captured through tree branching.\n",
    "# 4.Robust to outliers.\n",
    "# 5.Reduces overfitting and variance compared to a single decision tree.\n",
    "\n",
    "# Disadventages\n",
    "# 1.Prone to overfitting if the trees are too deep or complex.\n",
    "# 2.May not capture certain types of relationships well, such as linear relationships.\n",
    "# 3.Sensitive to noisy or mislabeled data, which can lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47acede-5257-4ed3-92f6-c40d1d0d0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 3:\n",
    "# 1. Bias: Random Forests are an ensemble of decision trees, and they inherit the bias characteristics of decision trees. \n",
    "#   They have the ability to capture complex relationships, resulting in relatively low bias.\n",
    "# 2.Variance: Random Forests aim to reduce the variance compared to individual decision trees. \n",
    "#  By training multiple trees on different subsets of the data and using feature randomness, \n",
    "#  they introduce diversity among the trees and reduce the variance in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503bf4da-6473-42c4-b80b-2f785a2a1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 4:\n",
    "# For classifiers\n",
    "# 1.The predictions of individual classifiers in the ensemble are combined using majority voting. \n",
    "#   The class label that receives the most votes is chosen as the final prediction.\n",
    "# 2.The predictions of individual regressors in the ensemble are combined using averaging. \n",
    "#   The average of all the predictions is considered as the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e97db8e-b9cf-410b-899c-cdbc2aa6e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 5 :\n",
    "# Ensemble size includes the number of models for the training dataset\n",
    "# 1.As the number of models in the ensemble increases, the variance tends to decrease. \n",
    "# Initially adding more models leads to a substantial reduction in variance, \n",
    "# but the improvement becomes marginal as the ensemble size increases further.\n",
    "# 2.As the ensemble size grows, so does the computational cost and memory requirements. \n",
    "# Large ensemble sizes may become computationally impractical, particularly if the base learner is computationally expensive or the dataset is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7405d6-0146-48b5-b887-73f1257316b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 6:\n",
    "# One example of a real-world application of bagging in machine learning is in the field of healthcare for the diagnosis of medical conditions. \n",
    "# Bagging can be used to create an ensemble of models to improve the accuracy and reliability of the diagnosis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
